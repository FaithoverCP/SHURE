{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FaithoverCP/SHURE/blob/main/Directional_AI_Arena_Alpha.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "directional_ai_intro"
      },
      "source": [
        "# Directional AI Arena Alpha\n",
        "This notebook showcases a minimal alpha version of the Directional AI Arena environment.\n",
        "\n",
        "## Features\n",
        "- Loads a base Language Model (GPT-2 by default, easily swappable)\n",
        "- Applies a *Directional AI Strategic Layer* to transform output\n",
        "- Inserts *IP trace metadata* for IP tracking (Invisa.ai™, SHURE®, IPIC)\n",
        "- Serves predictions via a *FastAPI* app with a public URL using *ngrok*\n",
        "\n",
        "## Next Steps\n",
        "1. **Run each cell in order.**\n",
        "2. *Replace the base model* if you have a private or specialized one.\n",
        "3. Connect to your front-end/UI or incorporate advanced logic for *enhanced reasoning signature*.\n",
        "4. Consider submission packages (LMSYS) or other expansions.\n",
        "\n",
        "Let's get started!"
      ],
      "id": "directional_ai_intro"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "install_dependencies"
      },
      "source": [
        "%%capture\n",
        "# 1) Install necessary packages\n",
        "!pip install fastapi nest-asyncio pyngrok uvicorn transformers torch\n"
      ],
      "execution_count": 38,
      "outputs": [],
      "id": "install_dependencies"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imports_and_setup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93b8b648-6a86-47c4-85b0-1e5936e88ed5"
      },
      "source": [
        "# 2) Import libraries\n",
        "import os\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "from fastapi import FastAPI, Body\n",
        "from pydantic import BaseModel\n",
        "import uvicorn\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# 3) Allow nested event loops for Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# 4) Set your ngrok auth token (REQUIRED)\n",
        "NGROK_AUTH_TOKEN = \"2uvVHqwJ7YY9U5PAgdomRCeVnzG_3mfBLKP5DLkdYPfT4WAEK\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# 5) Kill any running ngrok tunnels before starting a new one (Optional but recommended)\n",
        "ngrok.kill()\n",
        "\n",
        "# 6) Create a new public ngrok tunnel on port 8000\n",
        "public_url = ngrok.connect(8000)\n",
        "print(\"Ngrok tunnel URL:\", public_url)\n",
        "\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ngrok tunnel URL: NgrokTunnel: \"https://16c8b7159862.ngrok.app\" -> \"http://localhost:8000\"\n"
          ]
        }
      ],
      "id": "imports_and_setup"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "load_model",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0ee921a-905a-45e8-90eb-dd4cd43de47b"
      },
      "source": [
        "# 6) Load base model and tokenizer.\n",
        "#    By default, we use 'gpt2'. Replace with 'gpt2-medium', 'mistralai/Mistral-7B-v0.1', etc.\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "MODEL_NAME = \"gpt2\"  # Replace with your preferred model\n",
        "\n",
        "# Detect if GPU is available and set the appropriate device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load tokenizer and model\n",
        "print(f\"Loading model: {MODEL_NAME} ...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
        "\n",
        "# Create generation pipeline explicitly on the chosen device\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0 if device.type == 'cuda' else -1)\n",
        "\n",
        "print(\"✅ Model loaded successfully.\")\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Loading model: gpt2 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model loaded successfully.\n"
          ]
        }
      ],
      "id": "load_model"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "strategic_layer_heading"
      },
      "source": [
        "## 7) Directional AI Strategic Response Layer\n",
        "Here, we define a simple function to demonstrate how you can wrap or post-process the model’s output to align with **Directional AI** requirements.\n",
        "\n",
        "In a real production environment, this could involve specialized ranking, weighting, or dynamic re-routing of sub-prompts. For now, we’ll just do a placeholder function."
      ],
      "id": "strategic_layer_heading"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "strategic_layer",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0b6ced1-0d34-4b9a-ee08-def2baba62c8"
      },
      "source": [
        "def directional_ai_strategic_layer(raw_text: str) -> str:\n",
        "    \"\"\"Apply a simple transformation or marker to highlight the Directional AI strategic layer.\"\"\"\n",
        "    # Example: add a signature or transform the text in some way.\n",
        "    # For demonstration, we'll just add a prefix/suffix.\n",
        "    # In a real scenario, you might do more advanced logic.\n",
        "    prefix = \"[D-AI Strategic Output]\\n\"\n",
        "    suffix = \"\\n[End of D-AI Layer]\"\n",
        "    return f\"{prefix}{raw_text.strip()}{suffix}\"\n",
        "\n",
        "# Example usage:\n",
        "test_str = directional_ai_strategic_layer(\"This is a test of the Directional AI strategic layer.\")\n",
        "print(test_str)\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[D-AI Strategic Output]\n",
            "This is a test of the Directional AI strategic layer.\n",
            "[End of D-AI Layer]\n"
          ]
        }
      ],
      "id": "strategic_layer"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip_trace_metadata"
      },
      "source": [
        "## 8) IP Trace Metadata Injection\n",
        "We can embed relevant IP tracking data (Invisa.ai™, SHURE®, IPIC) in the response.\n",
        "\n",
        "For demonstration, we’ll attach a dictionary of metadata. In a real production setting, you’d likely store or transmit this metadata separately or embed it in a structured format like JSON."
      ],
      "id": "ip_trace_metadata"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "metadata_injection",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72ef3f1e-06f5-4355-d149-cc2548db785a"
      },
      "source": [
        "IP_TRACE_METADATA = {\n",
        "    \"Invisa.ai\": \"Proprietary Invisa.ai Markers\",\n",
        "    \"SHURE\": \"Strategic High-Utilization Reasoning Engine\",\n",
        "    \"IPIC\": \"Intellectual Property Innovation Coin\"\n",
        "}\n",
        "\n",
        "def inject_ip_metadata(text: str) -> str:\n",
        "    \"\"\"Append IP trace metadata to the output text.\"\"\"\n",
        "    metadata_str = \"\\n\\n[IP TRACE METADATA]\\n\" + \"\\n\".join(\n",
        "        f\"- {k}: {v}\" for k, v in IP_TRACE_METADATA.items()\n",
        "    )\n",
        "    return text + metadata_str\n",
        "\n",
        "# Example usage:\n",
        "sample_text = \"Sample output from the model.\"\n",
        "print(inject_ip_metadata(sample_text))\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample output from the model.\n",
            "\n",
            "[IP TRACE METADATA]\n",
            "- Invisa.ai: Proprietary Invisa.ai Markers\n",
            "- SHURE: Strategic High-Utilization Reasoning Engine\n",
            "- IPIC: Intellectual Property Innovation Coin\n"
          ]
        }
      ],
      "id": "metadata_injection"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fastapi_setup"
      },
      "source": [
        "## 9) FastAPI + ngrok Setup\n",
        "Below is a minimal FastAPI app with one POST endpoint `/predict`. You can send a JSON body containing a `prompt` and an optional `max_new_tokens` to generate text.\n",
        "\n",
        "### Example Request\n",
        "```bash\n",
        "curl -X POST \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -d '{\"prompt\":\"Hello, world!\"}' \\\n",
        "  <YOUR_NGROK_URL>/predict\n",
        "```\n",
        "### Response\n",
        "```json\n",
        "{\n",
        "  \"output\": \"[D-AI Strategic Output]\\n...\\n[End of D-AI Layer]\\n\\n[IP TRACE METADATA]\\n- Invisa.ai: ...\"\n",
        "}\n",
        "```"
      ],
      "id": "fastapi_setup"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fastapi_app"
      },
      "source": [
        "app = FastAPI()\n",
        "\n",
        "class PromptRequest(BaseModel):\n",
        "    prompt: str\n",
        "    max_new_tokens: int = 50\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "def predict(request: PromptRequest):\n",
        "    # Generate raw model output\n",
        "    generated = generator(\n",
        "        request.prompt,\n",
        "        max_new_tokens=request.max_new_tokens,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95\n",
        "    )\n",
        "    raw_text = generated[0]['generated_text']\n",
        "\n",
        "    # Pass through Directional AI strategic layer\n",
        "    strategic_text = directional_ai_strategic_layer(raw_text)\n",
        "\n",
        "    # Append IP trace metadata\n",
        "    final_text = inject_ip_metadata(strategic_text)\n",
        "\n",
        "    return {\"output\": final_text}\n"
      ],
      "execution_count": 45,
      "outputs": [],
      "id": "fastapi_app"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "start_server",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "268a3e99-341a-4066-e1dd-dd7c5117714a"
      },
      "source": [
        "# 10) Launch the FastAPI server asynchronously\n",
        "\n",
        "print(\"Starting FastAPI server... please wait.\")\n",
        "\n",
        "def run_app():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "# Run the server in the background (threaded)\n",
        "import threading\n",
        "server_thread = threading.Thread(target=run_app, daemon=True)\n",
        "server_thread.start()\n",
        "\n",
        "print(\"🚀 FastAPI server is running successfully!\")\n",
        "print(f\"🌐 Public URL: {public_url.public_url}/predict\")\n",
        "\n",
        "print(\"\\n✅ Example cURL command to test:\")\n",
        "print(f\"\"\"curl -X POST -H 'Content-Type: application/json' \\\\\n",
        "  -d '{{\"prompt\":\"Hello, Directional AI!\", \"max_new_tokens\":50}}' \\\\\n",
        "  {public_url.public_url}/predict\"\"\")\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting FastAPI server... please wait.\n",
            "🚀 FastAPI server is running successfully!\n",
            "🌐 Public URL: https://1d26174f3b8f.ngrok.app/predict\n",
            "\n",
            "✅ Example cURL command to test:\n",
            "curl -X POST -H 'Content-Type: application/json' \\\n",
            "  -d '{\"prompt\":\"Hello, Directional AI!\", \"max_new_tokens\":50}' \\\n",
            "  https://1d26174f3b8f.ngrok.app/predict\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [787]\n",
            "INFO:     Waiting for application startup.\n"
          ]
        }
      ],
      "id": "start_server"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "final_instructions"
      },
      "source": [
        "## You're All Set!\n",
        "1. **Run all cells** above.\n",
        "2. Your public *ngrok* URL will be displayed at the bottom.\n",
        "3. Use it for testing from anywhere or hooking up to your front-end.\n",
        "\n",
        "### Next Steps\n",
        "- **LMSYS Submission**: Once tested, let me know if you want the structured submission package.\n",
        "- **UI Layer**: We can build a Streamlit, Gradio, or custom front-end.\n",
        "- **Enhanced Reasoning Signature**: Integrate more advanced logic in the `directional_ai_strategic_layer()`.\n",
        "\n",
        "Let's put **Directional AI** on the map—time to lead the Arena!"
      ],
      "id": "final_instructions"
    }
  ]
}